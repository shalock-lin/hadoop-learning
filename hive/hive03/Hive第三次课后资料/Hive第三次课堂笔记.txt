å¥½ï¼Œæˆ‘è¿™è¾¹ç½‘è·¯æ˜¯okçš„

æœ‰ä»€ä¹ˆé—®é¢˜ç°åœ¨å¯ä»¥é—®ä¸€ä¸‹

èµ«èµ«å“ˆå˜¿ 19:50:35
æˆ‘ä»¬è¿™ä¸ªHiveå­¦å®Œèƒ½è¾¾åˆ°ä»€ä¹ˆæ°´å¹³ï¼Ÿ
æŠŠç›´æ’­è®²çš„hiveã€hiveå‡½æ•°ã€hiveæ¡ˆä¾‹ã€åŠåè¾¹5æ¬¡è¯¾çš„hiveæ•°ä»“æ¡ˆä¾‹ï¼Œéƒ½æŒæ¡ï¼Œå»ä¼ä¸šä¸­å®Œå…¨å¯ä»¥èƒœä»»æ•°ä»“å²—ä½çš„å·¥ä½œ
å½“ç„¶ï¼Œæœ‰äº›ä¸œè¥¿æ˜¯éœ€è¦åœ¨å·¥ä½œä¸­ç§¯ç´¯ã€æ‰©å±•çš„

hadoopç”Ÿæ€åœˆ
	hadoop
	hive
	hbase
	è¾…åŠ©æ¡†æ¶ï¼šæ•°æ®é‡‡é›†æ¡†æ¶flumeã€etlå·¥å…·sqoopã€è°ƒåº¦æ¡†æ¶azkaban
spark
flink

============
è¯¾å‰å›é¡¾ï¼š
åˆ†åŒºè¡¨
åˆ†æ¡¶è¡¨
	ä¸ºäº†æé«˜é‡‡æ ·çš„æ•ˆç‡
	æœ‰æ—¶éœ€è¦åšjoinæ“ä½œçš„æ—¶å€™ï¼Œè¯¾ä»¶é¿å…å…¨è¡¨æ•°æ®joinï¼Œæé«˜æ•ˆç‡
å¯¼å…¥æ•°æ®
å¯¼å‡ºæ•°æ®

æœ¬èŠ‚è¯¾ï¼šhiveçš„ä¼ä¸šçº§è°ƒä¼˜
hiveè¡¨çš„æ•°æ®å‹ç¼©
åœ¨é€‰å–å‹ç¼©ç®—æ³•çš„æ—¶å€™ï¼Œä¸»è¦è€ƒè™‘ä¸‰ä¸ªæ–¹é¢ï¼š
1ã€å‹ç¼©ç‡
2ã€å‹ç¼©çš„æ—¶é—´
3ã€å‹ç¼©ç®—æ³•æ—¶å€™æ”¯æŒåˆ‡åˆ†
åˆ‡åˆ†ï¼š
ä¸€ä¸ªæ–‡ä»¶ï¼Œç»è¿‡å‹è¿‡å‹ç¼©ç®—æ³•å‹ç¼©åï¼Œç»“æœä¸Šä¼ åˆ°hdfsï¼Œå®ƒä¹Ÿæ˜¯åˆ†å—å­˜å‚¨ï¼›300m-ã€‹3å—
ç„¶åï¼Œå¦‚æœæœ‰ä¸ªmrï¼Œä»¥æ­¤å‹ç¼©åçš„æ–‡ä»¶ä¸ºè¾“å…¥æ–‡ä»¶çš„è¯ï¼Œé‚£ä¹ˆ
çœ‹è¿™ä¸ªå‹ç¼©ç®—æ³•æ—¶å€™æ”¯æŒåˆ‡åˆ†
å¦‚æœæ”¯æŒï¼Œé‚£ä¹ˆï¼Œä¼šç”Ÿæˆ3ä¸ªmap task
å¦‚æœä¸æ”¯æŒï¼Œé‚£ä¹ˆåªä¼šæœ‰ä¸€ä¸ªmap taskï¼ˆå®ƒå°†3ä¸ªblockå—ä½œä¸ºä¸€ä¸ªmap taskçš„è¾“å…¥ï¼‰

set hive.exec.compress.intermediate=true;
hqlè¯­å¥ï¼Œç¼–è¯‘åç”Ÿæˆmrå‘¢ï¼Œå¯èƒ½æ˜¯æœ‰å¤šä¸ªmr
mr1 ->mr1çš„ç»“æœï¼ˆä¸­é—´ç»“æœï¼‰ mr2 -> æœ€ç»ˆç»“æœ
hive.exec.compress.intermediateå°±æ˜¯è®¾ç½®ï¼Œå°†ä¸­é—´ç»“æœåšå‹ç¼©

é›·å®æ‰¬ 20:21:55
snappyå‹ç¼©ï¼Œç”±äºä¸æ”¯æŒåˆ‡åˆ†ï¼Œè¿™æ ·ä¼šæ•°æ®å€¾æ–œå§ï¼Ÿ
æ¯”å¦‚è¯´ä¸€ä¸ªè¡¨çš„æ–‡ä»¶å¤§å°300ï¼ˆsnappyå‹ç¼©åçš„ï¼‰ï¼Œå¦‚æœ300æ˜¯ä¸€ä¸ªæ–‡ä»¶çš„è¯ï¼Œé‚£ä¹ˆåªæœ‰ä¸€ä¸ªmap task
åˆæƒ³ç”¨snappyå‹ç¼©ï¼Œåˆæƒ³è¦æ¯”è¾ƒå¥½çš„å¹¶è¡Œåº¦ï¼Œå¯ä»¥å¦‚ä½•ï¼Ÿ
å¯ä»¥å°†hiveçš„è¡¨æ–‡ä»¶å˜æˆå¤šä¸ª
128ã€128ã€44ä¸‰ä¸ªsnappyå‹ç¼©åçš„æ–‡ä»¶ï¼Œ=ã€‹3ä¸ªmap task

xxzx_5641682 20:24:12
hqlè®¾ç½®æ˜¯sessionçº§åˆ«çš„ 
åˆšæ‰å°†çš„å‘½ä»¤è¡Œé‡Œè®¾ç½®å±æ€§çš„ä¾‹å­ï¼Œå®ƒåªåœ¨å½“å‰sessionç”Ÿæ•ˆ
å¦‚æœæƒ³è¦æ°¸ä¹…ç”Ÿæ•ˆçš„è¯ï¼Œå¯ä»¥è®¾ç½®.hivercæ–‡ä»¶

æ–‡ä»¶å­˜å‚¨æ ¼å¼ï¼š
hiveè¡¨ï¼š
è¡Œå¼å­˜å‚¨
	textfile
	sequencefile
	ä¼˜ç‚¹ï¼š
		å¦‚æœæŸ¥è¯¢çš„æ—¶å€™ï¼Œè¦æŸ¥è¯¢å¤åˆæ¡ä»¶çš„æŸäº›è¡Œï¼ˆä¸€æ•´è¡Œï¼‰çš„æ—¶å€™ï¼Œæ•ˆç‡ä¼šé«˜ä¸€äº›
	åŠ£åŠ¿ï¼š
		å¦‚æœæ•°æ®ä¸­ï¼Œå­˜åœ¨å¤§é‡çš„nullçš„è¯ï¼Œè¿™äº›nullä¹Ÿä¼šå æ®å­˜å‚¨ç©ºé—´
åˆ—å¼å­˜å‚¨
	orc
	parquet
	ä¼˜ç‚¹ï¼š
		å¦‚æœæŸ¥è¯¢çš„æ—¶å€™ï¼ŒæŸ¥è¯¢ç‰¹å®šçš„åˆ—çš„è¯ï¼Œæ•ˆç‡é«˜
		ç›¸åŒåˆ—ï¼Œç›¸åŒç±»å‹çš„æ•°æ®å­˜å‚¨åœ¨ä¸€èµ·ï¼Œå‹ç¼©æ•ˆç‡æ›´é«˜
		
	ç¼ºç‚¹ï¼š

2017-08-10 13:00:00     http://www.taobao.com/item/962967_14?ref=1_1_52_search.ctg_1 T82C9WBFB1N8EW14YF2E2GY8AC9K5M5P http://www.yihaodian.com/ctg/s2/c24566-%E5%B1%
B1%E6%A5%82%E5%88%B6%E5%93%81?ref=pms_15_78_258 222.78.246.228  134939954       156

create table log_text (
track_time string,
url string,
session_id string,
referer string,
ip string,
end_user_id string,
city_id string
)ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE ;

create table log_orc(
track_time string,
url string,
session_id string,
referer string,
ip string,
end_user_id string,
city_id string)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
STORED AS orc ;

æ­¸å£¹ 20:38:42
åˆ—å¼å­˜å‚¨ï¼Œå¯ä»¥å–ä¸€è¡Œæ•°æ®å—ï¼Œæ‰€æœ‰å±æ€§ä¼šä¸ä¼šå¯¹åº”ä¸ä¸Šã€‚ï¼ˆç©ºä¸å­˜å‚¨ï¼‰
è‚¯å®šå¯ä»¥

create table log_parquet(
track_time string,
url string,
session_id string,
referer string,
ip string,
end_user_id string,
city_id string)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
STORED AS PARQUET ;  

èƒ–è™ãƒ¾ 20:43:49
è€å¸ˆï¼Œå¯ä»¥è¯´ä¸€ä¸‹ï¼Œå„ä¸ªå­˜å‚¨ã€å‹ç¼©çš„åº”ç”¨åœºæ™¯ä¹ˆ 
å­˜å‚¨æ ¼å¼ï¼š
	ä¼ä¸šä¸­ä¸€èˆ¬æœ‰orcæ ¼å¼
å‹ç¼©ï¼š
	ä¸€èˆ¬ä½¿ç”¨snappyã€lzoä¸¤ç§
	
å°æ˜åŒå­¦ 20:45:10
orc åªæ˜¯å­˜å‚¨æ–¹å¼æ˜¯åˆ—å­˜å‚¨å˜›ï¼Œ hqlæŸ¥è¯¢å•¥çš„éƒ½å’Œè¡Œå­˜å‚¨ä¸€æ ·å˜› 
hqlå¯ä»¥æŸ¥è¯¢hiveè¡¨ï¼ˆhiveè¡¨æœ‰è¡Œå¼å­˜å‚¨çš„ã€åˆ—å¼çš„ï¼‰

create table log_orc_none(
track_time string,
url string,
session_id string,
referer string,
ip string,
end_user_id string,
city_id string
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
STORED AS orc 
tblproperties ("orc.compress"="NONE");

è¡¨æ•°æ®çš„å„ä¸ªå­—æ®µé—´çš„åˆ†éš”ç¬¦æ˜¯å¤šä¸ªåˆ†å‰²ç¬¦çš„è¯ï¼Ÿ
\t
 
create table t1 (id String, name string)
row format serde 'org.apache.hadoop.hive.contrib.serde2.MultiDelimitSerDe'
WITH SERDEPROPERTIES ("field.delim"="##");

create  table t2(id int, name string)
row format serde 'org.apache.hadoop.hive.serde2.RegexSerDe' 
WITH SERDEPROPERTIES ("input.regex" = "^(.*)\\#\\#(.*)$");

æœªæ¥å·²æ¥ 20:53:14
å¯ä»¥é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼è®¾è®¡åŒæ—¶å¤„ç†å¤šç§åˆ†éš”ç¬¦å—ï¼Ÿæ¯”å¦‚åŒæ—¶å¤„ç† â€˜,â€™ å’Œ '\tâ€˜
è‚¯å®šæ˜¯ä¸è¡Œçš„
è¡¨ä¸­çš„å­—æ®µçš„åˆ†éš”ç¬¦ï¼Œåªèƒ½æ˜¯ä¸€ç§

hiveä¼ä¸šçº§è°ƒä¼˜
1ã€fetchæŠ“å–
hqlä¸­ï¼š
select * from stu;
select sid form stu;
æŸ¥è¯¢å¯ä»¥ä¸ç»è¿‡mrï¼Œå°±èƒ½è®²ç»“æœè¿”å›

2ã€æœ¬åœ°æ¨¡å¼
Mr.Jiang 20:59:01
ä¸ç»è¿‡MRï¼Œæ˜¯ç»è¿‡ä»€ä¹ˆè®¡ç®—ï¼Ÿ 
æœ¬åœ°è¿è¡Œ
ç±»ä¼¼ä¹‹å‰å­¦ä¹ mrçš„æ—¶å€™ï¼Œç›´æ¥åœ¨ideaä¸­è¿è¡Œmrï¼Œè¿™ä¸ªmrå°±æ˜¯ æœ¬åœ°è¿è¡Œ
è¿è¡Œæ¨¡å¼ï¼šæœ¬åœ°æ¨¡å¼ã€ä¼ªåˆ†å¸ƒå¼æ¨¡å¼ã€åˆ†å¸ƒå¼æ¨¡å¼

èƒ–è™ãƒ¾ 21:05:08
æœ¬åœ°è¿è¡Œä¸ºä»€ä¹ˆé€Ÿåº¦è¿™ä¸ªå¿«ï¼Ÿ 

join
è€ç‰ˆæœ¬ï¼šå°è¡¨ join å¤§è¡¨
å°è¡¨æ•°æ®é€‚åˆæ”¾åˆ°åˆ†å¸ƒå¼ç¼“å­˜çš„è¯ï¼Œä¹Ÿå¯ä»¥è€ƒè™‘map join

æ–°ç‰ˆæœ¬ï¼š
å°è¡¨ join å¤§è¡¨ å’Œ å¤§è¡¨ join å°è¡¨æ€§èƒ½æ²¡åŒºåˆ«

joinæ“ä½œçš„æ—¶å€™ï¼Œå¦‚æœæ˜¯å¤šè¡¨è¿›è¡Œjoinçš„è¯ï¼Œæœ€å¥½æ˜¯åˆ†å¼€å†™hqlè¯­å¥
a join b => join c

0       20111230000009  599cd26984f72ee68b2b6ebefccf6aed        å®‰å¾½åˆè‚¥365æˆ¿äº§ç½‘    http://hf.house365.com/

10      20111230000010  285f88780dd0659f5fc8acc7cc4949f2        IQæ•°ç   1       1    http://www.iqshuma.com/

use myhive;
create table ori(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) 
row format delimited fields terminated by '\t';

create table nullidtable(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\t';

create table jointable(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\t';

load data local inpath '/kkb/install/hivedatas/hive_big_table/*' into table ori; 
load data local inpath '/kkb/install/hivedatas/hive_have_null_id/*' into table nullidtable;

INSERT OVERWRITE TABLE jointable
SELECT a.* FROM nullidtable a JOIN ori b 
ON a.id = b.id;

mr -> keyç›¸åŒçš„æ•°æ®ï¼Œè¿›å…¥åˆ°ä¸€ä¸ªreduceä¸­åšå¤„ç†ï¼›
å¦‚æœaä¸­æœ‰ä¸€äº›id=nullï¼Œbä¸­ä¹Ÿæœ‰ä¸€äº›id=null
id=nullçš„è¿™äº›æ•°æ®æ˜¯ä¸æ˜¯key=nullï¼Œ=ã€‹ ä¸€ä¸ªreduceä¸­å¤„ç† å¤„ç†çš„æ•°æ®é‡æ¯”è¾ƒå¤§
æ•°æ®çš„å€¾æ–œ

å¦‚æœid=nullçš„æ•°æ®ï¼Œä¸æ˜¯æœ‰æ•ˆçš„æ•°æ®çš„è¯ï¼Œæˆ‘ä»¬å¯ä»¥åšè¿‡æ»¤æˆ–æ¸…æ´—ï¼Œå°†id=nullçš„æ•°æ®å…ˆè¿‡æ»¤æ‰ï¼Œç„¶åå†joinæ“ä½œï¼Œè¿™æ—¶æ•°æ®é‡å˜å°ï¼Œæ•ˆç‡æå‡

INSERT OVERWRITE TABLE jointable
SELECT a.* FROM (SELECT * FROM nullidtable WHERE id IS NOT NULL ) a JOIN ori b ON a.id = b.id;

ä½†æ˜¯æœ‰æ—¶ï¼šjoinæ¡ä»¶ä¸ºnullçš„æ•°æ®ï¼Œä¹Ÿæ˜¯æœ‰æ•ˆæ•°æ®ï¼Œè¿™æ—¶è¯¥æ€ä¹ˆåŠï¼Ÿ
id=nullçš„å€¼ï¼Œæ›¿æ¢ä¸€ä¸‹

set hive.exec.reducers.bytes.per.reducer=32123456;
set mapreduce.job.reduces=7;

INSERT OVERWRITE TABLE jointable
SELECT a.*
FROM nullidtable a
LEFT JOIN ori b 
ON CASE WHEN a.id IS NULL THEN 'hive' ELSE a.id END = b.id;
æ‰€æœ‰çš„id=nullå…¨éƒ¨æ›¿æ¢æˆhive
ä½†æ˜¯ï¼Œæ˜¯ä¸key=null =ã€‹ key=hive => æ•°æ®é‡è¿˜æ˜¯æ¯”è¾ƒå¤§ï¼Œè¿˜æ˜¯è¢«åŒä¸€ä¸ªreduceå¤„ç†
è¿˜æ˜¯æ•°æ®å€¾æ–œ

case when a then b when c then d else e end
 CASE WHEN a.id IS NULL THEN 'hive' ELSE a.id END = b.id;

INSERT OVERWRITE TABLE jointable
SELECT a.*
FROM nullidtable a
LEFT JOIN ori b ON CASE WHEN a.id IS NULL THEN concat('hive', rand()) ELSE a.id END = b.id;

CASE WHEN a.id IS NULL THEN concat('hive', rand()) ELSE a.id END 
id=null -> hive0000111
hive0000112
å˜æˆäº†ä¸åŒçš„key
ç¯èŠ‚æ•°æ®å€¾æ–œ

èµ«èµ«å“ˆå˜¿ 21:30:22
å¦‚æœæ˜¯æ•°æ®é‡å¾ˆå¤§ï¼Œè¦åšç¬›å¡å°”ç§¯è¯¥æ€ä¹ˆæ“ä½œï¼Ÿ 
ä¸€å®šè¦é¿å…ç¬›å¡å°”ä¹˜ç§¯æŸ¥è¯¢


 select  count(distinct ip )  from log_text;
 å¦‚æœæ•°æ®é‡å°çš„è¯ï¼Œå¯ä»¥å†™
 ä½†æ˜¯ï¼Œå¦‚æœæ•°æ®é‡å¤§çš„è¯ï¼Œå¼ºçƒˆåå¯¹ç”¨è¿™ç§åšæ³•
 åªå¯¹åº”ä¸€ä¸ªreduce task
 
 select count(ip) from 
 (select ip from log_text group by ip) t;
 æ•°æ®é‡å¤§çš„è¯ï¼Œæ€§èƒ½æ¯”è¾ƒå¥½
 
 1000*1000=1000000
 ä¸€å®šè¦é¿å…ç¬›å¡å°”ä¹˜ç§¯æŸ¥è¯¢
 
æ•°æ®å‰ªè£
å°±æ˜¯è¯´åšè¡¨æŸ¥è¯¢çš„æ—¶å€™ï¼Œå°½é‡å…ˆå°†è¦æŸ¥è¯¢çš„æ•°æ®é‡å‡å°

select * 
select sid æ€§èƒ½æ›´é«˜

å¦‚æœæ˜¯åˆ†åŒºè¡¨çš„è¯ï¼Œåªé’ˆå¯¹å…³å¿ƒçš„åˆ†åŒºçš„æ•°æ®åšæŸ¥è¯¢
where time='202002'

SELECT a.id
FROM bigtable a
LEFT JOIN ori b ON a.id = b.id
WHERE b.id <= 10;

SELECT a.id
FROM ori a
LEFT JOIN bigtable b ON (a.id <= 10 AND a.id = b.id);

0: jdbc:hive2://node03:10000> select * from order_partition;
Error: Error while compiling statement: FAILED: SemanticException [Error 10041]: No partition predicate found for Alias "order_partition" Table "order_partition" (state=42000,code=10041)

select * from order_partition where month='2019-03'; 

0: jdbc:hive2://node03:10000> select * from order_partition where month='2019-03' order by order_price; 
Error: Error while compiling statement: FAILED: SemanticException 1:61 In strict mode, if ORDER BY is specified, LIMIT must also be specified. Error encountered near token 'order_price' (state=42000,code=40000)

select * from order_partition where month='2019-03' order by order_price limit 3; 
 
explain
+----------------------------------------------------+--+
|                      Explain                       |
+----------------------------------------------------+--+
| STAGE DEPENDENCIES:                                |
|   Stage-1 is a root stage                          |
|   Stage-0 depends on stages: Stage-1               |
|                                                    |
| STAGE PLANS:                                       |
|   Stage: Stage-1                                   |
|     Map Reduce                                     |
|       Map Operator Tree:                           |
|           TableScan                                |
|             alias: order_partition                 |
|             Statistics: Num rows: 1 Data size: 189 Basic stats: PARTIAL Column stats: NONE |
|             Select Operator                        |
|               expressions: order_number (type: string), order_price (type: double), order_time (type: string), '2019-03' (type: string) |
|               outputColumnNames: _col0, _col1, _col2, _col3 |
|               Statistics: Num rows: 1 Data size: 189 Basic stats: PARTIAL Column stats: NONE |
|               File Output Operator                 |
|                 compressed: true                   |
|                 Statistics: Num rows: 1 Data size: 189 Basic stats: COMPLETE Column stats: NONE |
|                 table:                             |
|                     input format: org.apache.hadoop.mapred.TextInputFormat |
|                     output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat |
|                     serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe |
|                                                    |
|   Stage: Stage-0                                   |
|     Fetch Operator                                 |
|       limit: -1                                    |
|       Processor Tree:                              |
|         ListSink                                   |
|                                                    |
+----------------------------------------------------+--+

https://tech.meituan.com/2014/02/12/hive-sql-to-mapreduce.html
explain

 æ˜¯ä¸æ˜¯mapæ•°è¶Šå¤šè¶Šå¥½ï¼Ÿ
 è‚¯å®šä¸æ˜¯
 å¦‚æœæœ‰å¤§é‡çš„å°æ–‡ä»¶ -ã€‹è€ƒè™‘åˆå¹¶å°æ–‡ä»¶
 
 æ˜¯ä¸æ˜¯ä¿è¯æ¯ä¸ªmapå¤„ç†æ¥è¿‘128mçš„æ–‡ä»¶å—ï¼Œå°±é«˜æ•æ— å¿§äº†ï¼Ÿ
 ä¹Ÿä¸æ˜¯ï¼›
å°æ–‡ä»¶åˆå¹¶
CombineHiveInputFormat

æ ¹æ® ==computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))==å…¬å¼
è°ƒæ•´splitçš„å¤§å°-ã€‹è¿›è€Œè°ƒæ•´map taskçš„ä¸ªæ•°


å“²ç¨‹æ˜¯å¤§è…¿ğŸ˜˜ 22:11:06
åŠ¨æ€åˆ†åŒºéœ€è¦å¼€å¯éä¸¥æ ¼æ¨¡å¼ï¼Œä»£è¡¨åŠ¨æ€åˆ†åŒºæ“ä½œå±é™©ä¹ˆï¼Ÿ 
å¹¶éå¦‚æ­¤

PMKTEST 22:11:46
è¿™äº›å‚æ•°åº”è¯¥æ˜¯ä»£ç é‡Œé¢æŒ‡å®šçš„å§ 
æ¯”å¦‚ä¿®æ”¹map taskå¯æ•°
å¯ä»¥åœ¨hiveå‘½ä»¤é‡Œè¾¹ï¼šset mapreduce.input.fileinputformat.split.maxsize=10485760;
å¦‚æœæ˜¯mrç¼–ç¨‹çš„è¯ï¼Œä¹Ÿå¯ä»¥åœ¨ä»£ç é‡Œè®¾ç½®

èµ«èµ«å“ˆå˜¿ 22:11:58
ä¹‹å‰groupbyé‚£å—ï¼Œhiveå¯¹æ•°æ®è¿›è¡Œè´Ÿè½½å‡è¡¡ï¼Œhiveæ˜¯æ€ä¹ˆçŸ¥é“å‘ç”Ÿæ•°æ®å€¾æ–œäº†å‘¢ï¼Ÿ
hqlè¯­å¥è½¬æ¢æˆmr -ã€‹
è‚¯å®šçŸ¥é“å“ªäº›keyçš„æ•°æ®å¤šå•Š
å¦‚æœåˆ¤æ–­å‡ºæ¥æŸä¸ªkeyæ•°æ®é‡å¤šï¼Œä¼šå‡ºå‘è´Ÿè½½å‡è¡¡


å“²ç¨‹æ˜¯å¤§è…¿ğŸ˜˜ 22:13:39
æœ¬åœ°æ¨¡å¼å½“è¾“å…¥æ•°æ®é‡å°äºè¿™ä¸ªå€¼æ—¶é‡‡ç”¨local  mrçš„æ–¹å¼ï¼Œè€å¸ˆæ”¹çš„50000000ï¼Œä¾æ®æ˜¯ä»€ä¹ˆï¼Ÿç”Ÿäº§çš„æ—¶å€™æ€ä¹ˆç¡®å®šï¼Ÿ
æ²¡ä»€ä¹ˆä¾æ®
æ•°å€¼æ€ä¹ˆåˆ¶å®šï¼Œè¿™ä¸ªå¯ä»¥æ‰¾ä¸ªä¾‹å­ï¼Œåšä¸ªåŸºå‡†æµ‹è¯•ï¼ˆè€ƒè™‘å½“å‰é›†ç¾¤å„èŠ‚ç‚¹æ€§èƒ½ï¼‰
æ•°æ®è¾¾åˆ°å¤šå°‘æ—¶ï¼Œè¶…è¿‡æ­¤æ•°æ®é‡ï¼Œé›†ç¾¤è¿è¡Œåˆé€‚ï¼›å°äºçš„è¯ï¼Œæœ¬åœ°è¿è¡Œåˆé€‚

åˆ˜å‹‡-å¤§æ•°æ®005æœŸ 22:14:23
ä¼ä¸šé‡Œé¢ä¸€èˆ¬è®¾ç½®å‚æ•°æ—¶æ˜¯è®¾ç½®æˆæ°¸ä¹…ç”Ÿæ•ˆçš„è¿˜æ˜¯sessionçº§åˆ«çš„ 
å¦‚æœï¼Œç¡®å®šäº†æŸäº›å±æ€§éœ€è¦æ”¹ï¼Œè€Œä¸”æ˜¯åŸºæœ¬éƒ½ä½¿ç”¨çš„è¯ï¼Œå°±å†™æˆæ°¸ä¹…çš„
å¦åˆ™å°±æ˜¯ä¸´æ—¶
æœªæ¥å·²æ¥ 22:15:49
è¿™è‚¯å®šæ ¹æ®å®é™…æƒ…å†µæ¥ 

æ€»ç»“ï¼š
hiveçš„å­˜å‚¨æ ¼å¼
	è¡Œå¼å­˜å‚¨
		textfile
		sequencefile
	åˆ—å¼å­˜å‚¨
		orc ä¼ä¸šä¸­ç”¨çš„å¤š
		parquet
hiveçš„å‹ç¼©
	ä¼ä¸šä¸­å¸¸ç”¨çš„lzoã€snappy
	
hiveçš„è°ƒä¼˜

èµ«èµ«å“ˆå˜¿ 22:22:45
hiveç´¢å¼• 
é¸¡è‚‹ï¼Œç”¨çš„ä¸å¤š

æ—æ³½æ–Œ 22:23:33
è®¾ç½®æ°¸ä¹…ç”Ÿæ•ˆçš„å‚æ•°æ˜¯åœ¨å“ªé‡Œè®¾ç½®ï¼Ÿhive-site.xmlï¼Ÿ 
.hiverc
åœ¨HIVE_HOMEä¸‹åˆ›å»ºä¸€ä¸ª.hivercæ–‡ä»¶
æ¯”å¦‚è¦æƒ³æ°¸ä¹…è®¾ç½®reduceä¸ªæ•°ä¸º3
é‚£ä¹ˆåœ¨æ­¤æ–‡ä»¶ä¸­å†™ä¸Š
set mapreduce.job.reduces=3;

è¿›å…¥beeline
[hadoop@node03 hive-1.1.0-cdh5.14.2]$ pwd
/kkb/install/hive-1.1.0-cdh5.14.2
[hadoop@node03 hive-1.1.0-cdh5.14.2]$ beeline -i .hiverc
é‚£ä¹ˆåœ¨è¿›å…¥beelineå‰ï¼Œä¼šå…ˆæ‰§è¡Œ.hivercä¸­çš„hiveå‘½ä»¤
è¿æ¥åˆ°hiveserver2åï¼Œä¼šçœ‹åˆ°reduce ä¸ªæ•°å·²ç»è®¾ç½®ä¸º3
 set mapreduce.job.reduces;
 +--------------------------+--+
|           set            |
+--------------------------+--+
| mapreduce.job.reduces=3  |
+--------------------------+--+














 
 






















